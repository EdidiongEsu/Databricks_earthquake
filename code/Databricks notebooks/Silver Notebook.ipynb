{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, isnull, when\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9034c1ff-8a44-4c6f-a506-77ed9477d6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Data Factory\\nimport json\\n\\n# Retrieve the bronze_params directly as a widget\\nbronze_params = dbutils.widgets.get(\"bronze_params\")\\nprint(f\"Raw bronze_params: {bronze_params}\")\\n\\n# Parse the JSON string\\noutput_data = json.loads(bronze_params)\\n\\n# Access individual variables\\nstart_date = output_data.get(\"start_date\", \"\")\\nend_date = output_data.get(\"end_date\", \"\")\\nbronze_adls = output_data.get(\"bronze_adls\", \"\")\\nsilver_adls = output_data.get(\"silver_adls\", \"\")\\ngold_adls = output_data.get(\"gold_adls\", \"\")\\n\\nprint(f\"Start Date: {start_date}, Bronze ADLS: {bronze_adls}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Data Factory\n",
    "import json\n",
    "\n",
    "# Retrieve the bronze_params directly as a widget\n",
    "bronze_params = dbutils.widgets.get(\"bronze_params\")\n",
    "print(f\"Raw bronze_params: {bronze_params}\")\n",
    "\n",
    "# Parse the JSON string\n",
    "output_data = json.loads(bronze_params)\n",
    "\n",
    "# Access individual variables\n",
    "start_date = output_data.get(\"start_date\", \"\")\n",
    "end_date = output_data.get(\"end_date\", \"\")\n",
    "bronze_adls = output_data.get(\"bronze_adls\", \"\")\n",
    "silver_adls = output_data.get(\"silver_adls\", \"\")\n",
    "gold_adls = output_data.get(\"gold_adls\", \"\")\n",
    "\n",
    "print(f\"Start Date: {start_date}, Bronze ADLS: {bronze_adls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0bdd93-2899-4408-8210-796689aa6295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the JSON data into a Spark DataFrame\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}{start_date}_earthquake_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909ff42c-65b4-4dd2-8c41-198da8efa19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reshape earthquake data\n",
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated')\n",
    "    )\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243d96be-2db4-47f6-ad6c-ff94c239eee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data: Check for missing or null values\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('longitude', when(isnull(col('longitude')), 0).otherwise(col('longitude')))\n",
    "    .withColumn('latitude', when(isnull(col('latitude')), 0).otherwise(col('latitude')))\n",
    "    .withColumn('time', when(isnull(col('time')), 0).otherwise(col('time')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e311dc-bd48-47f8-8cfe-b0645b64c5ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'time' and 'updated' to timestamp from Unix time\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('time', (col('time') / 1000).cast(TimestampType()))\n",
    "    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bffcd10a-c21d-40f2-85e5-058df2627f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the transformed DataFrame to the Silver container\n",
    "silver_output_path = f\"{silver_adls}earthquake_events_silver/\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49153fea-7f31-4dd8-b927-01a79e510c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append DataFrame to Silver container in Parquet format\n",
    "df.write.mode('append').parquet(silver_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cdd8b5-bfd8-40dc-b8a0-a6f275237f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Data Factory\\ndbutils.notebook.exit(silver_output_path)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##  Data Factory                                                                \n",
    "dbutils.notebook.exit(silver_output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
